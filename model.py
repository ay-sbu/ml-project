# -*- coding: utf-8 -*-
"""ML_Project_V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F-aow6wba_iZ2l5T9jUiH6FHEYiCWI3u
"""

'''
Machine Learning Project
Abbas Yazdan Mehr - Reyhane Naseri Moghadam
'''

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

"""**Feature Engineering**"""

data =pd.read_csv("drive/My Drive/Machine Learning/FinalProject/dataset.csv")

# Check the properties of each column of dataset
'''
Results:
*   total number of rows : 1152
1.  need to deal with missing values :
    hdd          -> 188
    graphic_ram  -> 896
    ssd          -> 956
2.  If we have fixed the missing values,
    we will have at least "1037" rows of data to check

3.  drop the "link" column
4.  make "price" column as target

5.  cpu : one-hot encoding
6. standardize the price values
7. stock_status : categorical -> numerical

8. omit "gb/tb/mb" -> ram,hdd,ssd,graphic_ram , "inch" -> screen_size
'''
data.info()

# Step-1: Drop the "link" column
data = data.drop(['link'], axis=1)

'''
# Step-2 : Take the values of "price" column as target
target =  list(data["price"])
'''
# Step-2: Replacement

# Part.1 : convert

# NaN     -> 0 (in every column)
# data = data.replace(np.nan,0,regex=True)
# unified -> 0 (in graphic_ram)
data['graphic_ram'] = data['graphic_ram'].str.replace('unified','0')

# Part.2 : omit

# ram,hdd,ssd,graphic_ram -> omit "gb/tb"
data = data.replace('gb','',regex=True)
data = data.replace('tb','',regex=True)
data = data.replace('mb','',regex=True)
data = data.replace('kb','',regex=True)
# screen_size -> omit "inch"
data['screen_size'] = data['screen_size'].str.replace('inch','')

# Step-3 : Imputing the Missing Values
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
item_list = list(data.columns.values)
# Attention that 'cpu' and 'stock_status' are categorical and haven't changed to numerical yet
column_list = [e for e in item_list if e not in ('cpu', 'stock_status')]
for column in column_list:
  data[column] = imp.fit_transform(data[column].values.reshape(-1, 1))

imp = SimpleImputer(missing_values=0.0, strategy='most_frequent')
data['graphic_ram'] = imp.fit_transform(data['graphic_ram'].values.reshape(-1, 1) )

data.head(10)

# Step-4 :  One-Hot Encoding (Alternative : Lable Encoding)

data = pd.get_dummies(data, columns = ['cpu'])
data['stock_status'] = data['stock_status'].astype('category').cat.codes # !!! IT CAN GET DESCENDING !!!

"""
Alternative :
# Label Encoding for "cpu" values (faster way)
data['cpu'] = data['cpu'].astype('category').cat.codes
data = pd.get_dummies(data, columns = ['stock_status'])

# Another way for One-Hot encoding with sklearn
from sklearn.preprocessing import OneHotEncoder
enc_data = OneHotEncoder().fit_transform(['cpu']).toarray()
data = data.join(enc_data)
"""

# Step-5 : Standardization the price values
import scipy
from scipy import stats
# Z-Score using scipy
data['price'] = stats.zscore(data['price'])

data.head(10)

item_list = list(data.columns.values)
# Attention that 'cpu' and 'stock_status' are categorical and haven't changed to numerical yet
column_list = [e for e in item_list if e not in ('price')]

X = data[column_list]
y = data['price']

# here we use the "train_test_split" to divide our dataset into train and test set -> train=0.7 dataset + test=0.3 dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

regr = RandomForestRegressor(max_depth=20, random_state=20)
regr.fit(X_train, y_train)
predictions = regr.predict(X_test)
score = regr.score(X_test,y_test)
print(score)

"""# XGBoost"""

from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor

le = LabelEncoder()
y_train = le.fit_transform(y_train)

model = XGBRegressor(n_estimators = 400, learning_rate = 0.0000001, max_depth = 10)
model.fit(X_train.values, y_train)

y_pred = model.predict(X_test)

from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("mse: ", mse)
print("rmse: ", rmse)